{
# # Core/agent_create_v1.py
# from langchain_classic.agents import AgentExecutor
# from Core.tools import dispalyed_flighs
# from Config.models_config import default_model
# from Config.settings import OPENROUTER_KEY
# from langchain_core.prompts import PromptTemplate
# from langchain_classic import hub
# from langchain_openai import ChatOpenAI        
# from langchain.agents import create_agent
# from langchain_classic.agents import initialize_agent, AgentType


# # â† Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø­Ø±ÙŠ (Ø³Ø·Ø±ÙŠÙ† Ø¨Ø³)
# llm = ChatOpenAI(
#     model=default_model,                     # Ù…Ø«Ù„Ø§Ù‹: "deepseek/deepseek-chat"
#     api_key=OPENROUTER_KEY,                  # Ù…ÙØªØ§Ø­Ùƒ Ù…Ù† OpenRouter
#     base_url="https://openrouter.ai/api/v1", # Ø¯Ù‡ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ®Ù„Ù‘ÙŠÙ‡ ÙŠØ±ÙˆØ­ Ù„Ù€ OpenRouter
#     temperature=0,
#     max_tokens=4096,                         # Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
# )
# # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹
# #print(llm.invoke("Hi").content)
# #----------------------------------------------------------------------
# prompt = hub.pull("hwchase17/react")
# template="explain the concept:{concept}"
# pt=PromptTemplate.from_template(template=template)
# # prompt=pt.invoke({"concept":"Prompting LLMS"})
# # print(prompt)
# chain= pt | llm
# concept="American Football"
# # print(chain.invoke({"concept":concept}))
# agent = initialize_agent(
#     tools=[dispalyed_flighs],
#     llm=llm,
#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
#     verbose=True,
#     handle_parsing_errors=True,
# )

# # message=agent.invoke({"message":[("human","search for flights")]})
# agent_executor = AgentExecutor(
#     agent=agent,
#     tools=[dispalyed_flighs],
#     verbose=True,                      # Ø¹Ø´Ø§Ù† ØªØ´ÙˆÙ Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©
#     handle_parsing_errors=True,
#     max_iterations=15,
# )
# response = agent.invoke({
#     "input": "search for flights"
# })
# print(response["output"])


# # # --- tools: use your @tool decorated function directly ---
# # tools = [dispalyed_flighs]

# # # --- create the agent (langchain v1 style) ---
# # agent = create_agent(
# #     model=llm,                      # pass the LLM instance
# #     tools=tools,
# #     system_prompt="You are an agent that must call tools to find flight offers. Use the 'search_flights' tool with origin, destination, date.",
# # )

# # # --- flexible invocation helper (some v1 objects use invoke, others run) ---
# # def run_agent_query(agent_obj, query: str):
# #     # prefer invoke({"input": ...}) (v1-style)
# #     if hasattr(agent_obj, "invoke") and callable(agent_obj.invoke):
# #         out = agent_obj.invoke({"input": query})
# #         # often returns dict with "output"
# #         if isinstance(out, dict) and "output" in out:
# #             return out["output"]
# #         return out
# #     # fallback to run(query)
# #     if hasattr(agent_obj, "run") and callable(agent_obj.run):
# #         return agent_obj.run(query)
# #     # last resort: call the agent object
# #     if callable(agent_obj):
# #         return agent_obj(query)
# #     raise RuntimeError("Can't invoke agent with known patterns")

# # if __name__ == "__main__":
# #     q = "Find cheapest flights from CAI to DXB on 2026-11-10"
# #     print("Query:", q)
# #     res = run_agent_query(agent, q)
# #     print("\n=== AGENT RESULT ===\n")
# #     # pretty print if dict
# #     if isinstance(res, dict):
# #         print(json.dumps(res, indent=2, ensure_ascii=False))
# #     else:
# #         print(res)
}
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from Graduation_Project.Core.search_flights.tools import search_for_flights
from Config.settings import OPENROUTER_KEY , GEMINI_KEY
from Config.models_config import default_model

# âœ… Ø§Ø³ØªØ®Ø¯Ù… Llama 3.1 Ø¨Ø¯Ù„ DeepSeek (Ù…Ø¬Ø§Ù†ÙŠ ÙˆØ¨ÙŠØ¯Ø¹Ù… Tools)
llm = ChatOpenAI(
    model=default_model,  
    api_key=OPENROUTER_KEY,
    base_url="https://openrouter.ai/api/v1",
    temperature=0,
    max_tokens=4096,
    streaming=False,
)
# Ø§Ø®ØªØ¨Ø§Ø±
print("ğŸ”„ Testing LLM...")
test_response = llm.invoke("Say 'Hello, I am ready!'")
print(test_response.content)
print("\nâœ… LLM is working!\n")

# ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Agent
print("ğŸ¤– Creating agent...")
agent = create_agent(llm, tools=[search_for_flights])

# ØªØ´ØºÙŠÙ„
print("âœˆï¸ Searching for flights...\n")
result = agent.invoke({
    "messages": [("system", "You are a helpful travel assistant. Always return clean JSON flight results with prices and departure times. also make sure the date is in this format YEAR-Month-DAY"),
                 ("user", "search for flights from new york city to singapore in 27 november 2025")]
})

# Ø§Ù„Ù†ØªÙŠØ¬Ø©
print("\nğŸ“‹ Final Result:")
print(result["messages"][-1].content)